{
 "metadata": {
  "name": "250 Parallelize symbolic mathematics on multi-dimensional arrays with Theano"
 },
 "nbformat": 3,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "source": [
      "# Theano parallelizes symbolic mathematics on multi-dimensional arrays",
      "",
      "Research in artificial intelligence involves experimenting with different ways to maximize learning and minimize error.  [Theano](http://deeplearning.net/software/theano) accelerates the experimentation process by making it easy to define, evaluate, differentiate functions on multi-dimensional arrays, with the optional benefit of using GPUs for speedups of up to 140x."
     ]
    },
    {
     "cell_type": "code",
     "input": [
      "import theano",
      "import theano.tensor as T"
     ],
     "language": "python",
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "Using gpu device 0: ION",
        ""
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "source": [
      "## Compile symbolic mathematics"
     ]
    },
    {
     "cell_type": "markdown",
     "source": [
      "Add two scalars after defining and compiling a function."
     ]
    },
    {
     "cell_type": "code",
     "input": [
      "x = T.dscalar('x') # Decimal scalar (float32)",
      "y = T.dscalar('y')",
      "f = theano.function([x, y], x + y)",
      "f(2, 3)"
     ],
     "language": "python",
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 12,
       "text": [
        "array(5.0)"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "input": [
      "f"
     ],
     "language": "python",
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 13,
       "text": [
        "<theano.compile.function_module.Function at 0xb8bf60c>"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "source": [
      "Add two matrices after defining and compiling a function."
     ]
    },
    {
     "cell_type": "code",
     "input": [
      "x = T.dmatrix('x')",
      "y = T.dmatrix('y')",
      "f = theano.function([x, y], x + y)",
      "f([[1, 2], [3, 4]], [[-1, -2], [-3, -4]])"
     ],
     "language": "python",
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 26,
       "text": [
        "array([[ 0.,  0.],",
        "       [ 0.,  0.]])"
       ]
      }
     ],
     "prompt_number": 26
    },
    {
     "cell_type": "markdown",
     "source": [
      ""
     ]
    },
    {
     "cell_type": "code",
     "input": [
      "state = theano.shared(0)",
      "x = T.iscalar('x') # Integer scalar (int32)",
      "accumulate = theano.function([x], state, updates=[(state, state + x)])",
      "accumulate(10); print state.get_value()",
      "accumulate(20); print state.get_value()"
     ],
     "language": "python",
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "10",
        "30",
        ""
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "source": [
      "## Differentiate functions on multi-dimensional arrays"
     ]
    },
    {
     "cell_type": "code",
     "input": [
      "x = T.dscalar('x')",
      "y = x ** 2",
      "f = theano.function([x], T.grad(y, x))",
      "print 'The derivative of %s is %s.' % (",
      "    theano.pp(y),",
      "    theano.pp(f.maker.env.outputs[0]))",
      "print 'Evaluating the derivative at x = 4 gives %s.' % f(4)"
     ],
     "language": "python",
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "The derivative of (x ** TensorConstant{2}) is (TensorConstant{2.0} * x).",
        "Evaluating the derivative at x = 4 gives 8.0.",
        ""
       ]
      }
     ],
     "prompt_number": 28
    },
    {
     "cell_type": "code",
     "input": [
      "x = T.dmatrix('x')",
      "s = T.sum(1 / (1 + T.exp(-x)))",
      "gs = T.grad(s, x)",
      "dlogistic = theano.function([x], gs)",
      "dlogistic([[0, 1], [-1, -2]])"
     ],
     "language": "python",
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 2,
       "text": [
        "array([[ 0.25      ,  0.19661193],",
        "       [ 0.19661193,  0.10499359]])"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "input": [
      "f = theano.function([x], T.jacobian(s, x))",
      "f([[0, 1], [-1, -2]])"
     ],
     "language": "python",
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 8,
       "text": [
        "array([[ 0.25      ,  0.19661193],",
        "       [ 0.19661193,  0.10499359]])"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "source": [
      "## Use GPUs for speedups of up to 140x"
     ]
    },
    {
     "cell_type": "code",
     "input": [
      "from theano import function, config, shared, sandbox",
      "import theano.tensor as T",
      "import numpy",
      "import time",
      "",
      "vlen = 10 * 30 * 768  # 10 x #cores x # threads per core",
      "iters = 1000",
      "",
      "rng = numpy.random.RandomState(22)",
      "x = shared(numpy.asarray(rng.rand(vlen), config.floatX))",
      "f = function([], T.exp(x))",
      "t0 = time.time()",
      "for i in xrange(iters):",
      "    r = f()",
      "print 'Looping %d times took'%iters, time.time() - t0, 'seconds'",
      "print 'Result is', r",
      "print 'Used the','cpu' if numpy.any( [isinstance(x.op,T.Elemwise) for x in f.maker.env.toposort()]) else 'gpu'"
     ],
     "language": "python",
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Looping 1000 times took 1.57673311234 seconds",
        "Result is <CudaNdarray object at 0xcc0f2f0>",
        "Numpy result is [ 1.23178029  1.61879349  1.52278066 ...,  2.20771813  2.29967761",
        "  1.62323296]",
        "Used the gpu",
        ""
       ]
      }
     ],
     "prompt_number": 29
    },
    {
     "cell_type": "code",
     "input": [
      "from theano import function, config, shared, sandbox, Out",
      "import theano.tensor as T",
      "import numpy",
      "import time",
      "",
      "vlen = 10 * 30 * 768  # 10 x #cores x # threads per core",
      "iters = 1000",
      "",
      "rng = numpy.random.RandomState(22)",
      "x = shared(numpy.asarray(rng.rand(vlen), config.floatX))",
      "f = function([],",
      "        Out(sandbox.cuda.basic_ops.gpu_from_host(T.exp(x)),",
      "            borrow=True))",
      "t0 = time.time()",
      "for i in xrange(iters):",
      "    r = f()",
      "print 'Looping %d times took'%iters, time.time() - t0, 'seconds'",
      "print 'Result is', r",
      "print 'Numpy result is', numpy.asarray(r)",
      "print 'Used the','cpu' if numpy.any( [isinstance(x.op,T.Elemwise) for x in f.maker.env.toposort()]) else 'gpu'"
     ],
     "language": "python",
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "source": [
      "## Learn more",
      "",
      "- [Theano](http://deeplearning.net/software/theano)",
      "- [Theano Tutorial](http://deeplearning.net/software/theano/tutorial/index.html#tutorial)",
      "- [Theano Code Repository](https://github.com/Theano/Theano)",
      "- [Theano Code Examples](https://github.com/Theano/Theano/tree/master/doc/examples)",
      "- [Theano Code Usage Tests](https://github.com/Theano/Theano/tree/master/theano/tests)",
      "- [Deep Learning](http://www.deeplearning.net)",
      "- [Deep Learning Tutorial](http://www.deeplearning.net/tutorial)",
      "- [Deep Learning Tutorial Code Repository](https://github.com/lisa-lab/DeepLearningTutorials)",
      "- [Deep Learning Tutorial Code Examples](https://github.com/lisa-lab/DeepLearningTutorials/tree/master/code)",
      "- [Lisa Lab at the University of Montr\u00e9al](http://www.iro.umontreal.ca/rubrique.php3?id_rubrique=27)"
     ]
    }
   ]
  }
 ]
}